{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "6d3a7e16-451c-41dd-ca60-64a31c2a49f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "5e1efe90-84b2-4baa-94e4-89ec565de20a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 665Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 559kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 299Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:53, 9.38Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 678Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 856kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 847kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "7586adde-2d14-433f-e884-9825a7546406"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "c0833824-b4c4-4631-c9ed-425a8885fe6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? There's no place like it.\n",
            "\n",
            "Explore further: A new species of dinosaur could be found near the Moon<|endoftext|>Steampunk is a genre that has been around for decades. In that time, it has been re-imagined as a way\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "bc524c89-81cb-422f-f9e7-0598238271ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:36:44--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.132.232, 52.217.201.216, 52.217.171.80, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.132.232|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txt.2â€™\n",
            "\n",
            "nietzsche.txt.2     100%[===================>] 586.82K   537KB/s    in 1.1s    \n",
            "\n",
            "2023-05-24 11:36:46 (537 KB/s) - â€˜nietzsche.txt.2â€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "3135f0eb-dbea-46a1-82ee-36c5e96da35f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 124MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "90cf9f00-805e-48bd-a67f-3e41a4e3b27a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:36:52--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-05-24 11:36:52 (144 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "258585e5-2946-4ae9-f687-e35a7ce05e18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 6.91] loss=3.42 avg=3.42\n",
            "[2 | 8.95] loss=3.43 avg=3.43\n",
            "[3 | 11.01] loss=3.47 avg=3.44\n",
            "[4 | 13.06] loss=3.37 avg=3.42\n",
            "[5 | 15.12] loss=3.52 avg=3.44\n",
            "[6 | 17.18] loss=3.37 avg=3.43\n",
            "[7 | 19.26] loss=3.30 avg=3.41\n",
            "[8 | 21.32] loss=3.26 avg=3.39\n",
            "[9 | 23.39] loss=3.11 avg=3.36\n",
            "[10 | 25.45] loss=3.30 avg=3.35\n",
            "[11 | 27.54] loss=3.28 avg=3.35\n",
            "[12 | 29.61] loss=3.23 avg=3.34\n",
            "[13 | 31.69] loss=3.18 avg=3.32\n",
            "[14 | 33.78] loss=3.22 avg=3.32\n",
            "[15 | 35.86] loss=3.14 avg=3.30\n",
            "[16 | 37.95] loss=3.17 avg=3.29\n",
            "[17 | 40.04] loss=3.14 avg=3.28\n",
            "[18 | 42.12] loss=3.07 avg=3.27\n",
            "[19 | 44.22] loss=3.32 avg=3.27\n",
            "[20 | 46.31] loss=3.17 avg=3.27\n",
            "[21 | 48.41] loss=3.19 avg=3.26\n",
            "[22 | 50.51] loss=3.18 avg=3.26\n",
            "[23 | 52.62] loss=3.13 avg=3.25\n",
            "[24 | 54.74] loss=3.14 avg=3.25\n",
            "[25 | 56.85] loss=3.07 avg=3.24\n",
            "[26 | 58.96] loss=3.01 avg=3.23\n",
            "[27 | 61.07] loss=3.09 avg=3.22\n",
            "[28 | 63.19] loss=2.98 avg=3.21\n",
            "[29 | 65.32] loss=3.04 avg=3.21\n",
            "[30 | 67.44] loss=3.12 avg=3.20\n",
            "[31 | 69.57] loss=3.12 avg=3.20\n",
            "[32 | 71.70] loss=3.18 avg=3.20\n",
            "[33 | 73.84] loss=2.95 avg=3.19\n",
            "[34 | 75.98] loss=2.91 avg=3.18\n",
            "[35 | 78.13] loss=3.11 avg=3.18\n",
            "[36 | 80.27] loss=3.06 avg=3.18\n",
            "[37 | 82.41] loss=3.04 avg=3.17\n",
            "[38 | 84.56] loss=3.17 avg=3.17\n",
            "[39 | 86.71] loss=3.08 avg=3.17\n",
            "[40 | 88.87] loss=2.98 avg=3.16\n",
            "[41 | 91.03] loss=2.92 avg=3.16\n",
            "[42 | 93.19] loss=2.84 avg=3.15\n",
            "[43 | 95.35] loss=3.00 avg=3.14\n",
            "[44 | 97.51] loss=3.08 avg=3.14\n",
            "[45 | 99.68] loss=2.97 avg=3.14\n",
            "[46 | 101.85] loss=3.14 avg=3.14\n",
            "[47 | 104.02] loss=2.96 avg=3.13\n",
            "[48 | 106.19] loss=2.93 avg=3.13\n",
            "[49 | 108.36] loss=3.05 avg=3.12\n",
            "[50 | 110.54] loss=2.81 avg=3.12\n",
            "[51 | 112.73] loss=3.04 avg=3.11\n",
            "[52 | 114.91] loss=2.89 avg=3.11\n",
            "[53 | 117.10] loss=3.06 avg=3.11\n",
            "[54 | 119.27] loss=2.98 avg=3.10\n",
            "[55 | 121.46] loss=3.04 avg=3.10\n",
            "[56 | 123.65] loss=3.09 avg=3.10\n",
            "[57 | 125.84] loss=2.91 avg=3.10\n",
            "[58 | 128.04] loss=2.94 avg=3.09\n",
            "[59 | 130.23] loss=2.99 avg=3.09\n",
            "[60 | 132.43] loss=2.93 avg=3.09\n",
            "[61 | 134.64] loss=3.03 avg=3.09\n",
            "[62 | 136.85] loss=2.89 avg=3.08\n",
            "[63 | 139.05] loss=2.90 avg=3.08\n",
            "[64 | 141.26] loss=3.01 avg=3.08\n",
            "[65 | 143.48] loss=2.81 avg=3.07\n",
            "[66 | 145.69] loss=2.99 avg=3.07\n",
            "[67 | 147.91] loss=2.80 avg=3.06\n",
            "[68 | 150.14] loss=3.04 avg=3.06\n",
            "[69 | 152.36] loss=2.84 avg=3.06\n",
            "[70 | 154.59] loss=2.90 avg=3.06\n",
            "[71 | 156.82] loss=2.95 avg=3.05\n",
            "[72 | 159.05] loss=3.11 avg=3.06\n",
            "[73 | 161.28] loss=3.07 avg=3.06\n",
            "[74 | 163.52] loss=2.71 avg=3.05\n",
            "[75 | 165.75] loss=2.88 avg=3.05\n",
            "[76 | 167.98] loss=2.87 avg=3.04\n",
            "[77 | 170.21] loss=2.77 avg=3.04\n",
            "[78 | 172.45] loss=2.80 avg=3.03\n",
            "[79 | 174.68] loss=2.92 avg=3.03\n",
            "[80 | 176.91] loss=2.82 avg=3.03\n",
            "[81 | 179.14] loss=3.01 avg=3.03\n",
            "[82 | 181.37] loss=2.89 avg=3.02\n",
            "[83 | 183.59] loss=2.83 avg=3.02\n",
            "[84 | 185.82] loss=2.85 avg=3.02\n",
            "[85 | 188.05] loss=2.80 avg=3.01\n",
            "[86 | 190.27] loss=2.93 avg=3.01\n",
            "[87 | 192.49] loss=2.74 avg=3.01\n",
            "[88 | 194.71] loss=2.85 avg=3.01\n",
            "[89 | 196.93] loss=2.82 avg=3.00\n",
            "[90 | 199.15] loss=3.04 avg=3.00\n",
            "[91 | 201.36] loss=2.80 avg=3.00\n",
            "[92 | 203.57] loss=2.99 avg=3.00\n",
            "[93 | 205.78] loss=2.85 avg=3.00\n",
            "[94 | 207.99] loss=2.78 avg=2.99\n",
            "[95 | 210.21] loss=2.84 avg=2.99\n",
            "[96 | 212.42] loss=2.87 avg=2.99\n",
            "[97 | 214.63] loss=2.84 avg=2.99\n",
            "[98 | 216.84] loss=2.92 avg=2.99\n",
            "[99 | 219.06] loss=2.80 avg=2.98\n",
            "[100 | 221.27] loss=2.83 avg=2.98\n",
            "======== SAMPLE 1 ========\n",
            " Hagon the Bear. This is a good thing; the Dothraki need no more of a healer than they do of a man.\" A man named Tullor walked around the room with a beard almost matted, and his beard seemed to grow and heal. There were more people on that bench than were there in the Vale. \"Let's make the rest of them.\" The old man said. He had a short beard; black, yet it was a fine one. \"I like the grey,\" he said. \"We ought to see if this hurts.\" \n",
            "The old man turned their faces away. \"I did not see what you were talking about,\" he said \n",
            ", \"but the good man needs to do his training first and get the good man out of there.\" \n",
            "Beside all of them, they were the most frightened the world had ever seen. He made the bear stand on its face, and the \n",
            "Lannisters pushed him to the floor. \"The boy is no good,\" the old man said. The bear crumbled on impact, and the \n",
            "healer was thrown to the ground. Tullor looked down at the bench floor and went on, \"Tullor, you should have been the one to make \n",
            "it.\" The old man looked away. \"I will not be here,\" he said. \n",
            "Tullor was so terrified that he had to lift himself up. Jhogo looked down at the ground, but at the same time almost frightened. \n",
            "\"No,\" Jhogo warned him. \"This is not my place.\" \n",
            "Ser Barristan Selmy, the Hand of the King, was almost as shocked. \"You do not want to go out, are you?\" his voice sounded so \n",
            "cracking it was hard to get out of place. The old man's voice. \"No,\" he started, \"but I want to go. I want to see Jhogo, I want to see, see what \n",
            "he's like.\" \n",
            "Jhogo's expression was uncertain. \"Ser Barristan Selmy,\" he said, \"this is not safe.\" \n",
            "\"If you want to see you should first go to the north,\" Ser Barristan said. \"The Dragon's Watch is not safe. The rest of Jhiqui \n",
            "is not safe. So we must send south.\" They were marching west at once when the old man said it. \n",
            "Page \n",
            "A storm rippled in the darkness and Jhogo was running off and not far from the tower. The dwarf said he could not make it out \n",
            "yet; he was sick. \n",
            "The old man took up his leave of the King's Landing. He had left his horse to go west to the Blackwater \n",
            "River. \"There is no way,\" the old man said. He turned the wagon to the rear and moved the horses north, \n",
            "hoping it would help him. \n",
            "Ser Barristan Selmy was anxious. \n",
            "The rest of the Way of Kings was a maze of dark grey, black stone, and barred and matted with smoke. The \n",
            "road to the Blackwater was treacherous. \"There will be people around here,\" the old man said, \"and more than one \n",
            "rider. It is a great distance from here.\" \n",
            "\"I see,\" Jhogo said. \"As you told him.\" \n",
            "Page\n",
            "\n",
            "There were a small handful of them, he knew. They were armed with spears, but they were nothing close to \n",
            "the most formidable of the guardsmen he had summoned. The \n",
            "dwarf-beasts with their long horns would not have fled without their lances; he wanted only to go south into \n",
            "the Blackwater before the rest of them moved in. The old man had three lances, a hundred of them, and enough \n",
            "strength to cut a path through two enemy-held villages, but not enough for them to \n",
            "jump out atop their lances. His men moved fast and determinedly, with a thousand swords on them, to the south of he \n",
            "Tullor, where the road to the river had to narrow in order for the Dothraki to cross. The Dothraki did not \n",
            "want to cross over here; the river could not be passed. The old man heard a noise, and felt something move. \n",
            "It came back to him; \n",
            "they were only a few hundred feet from the road. The Dothraki were running at a gallop and they were \n",
            "running at a halt; what would be a standstill would not be. His old friend was gone, and he could see his \n",
            "riders scrambling toward the edge of the road, their riders slinging spears across the black earth as the Dothraki moved \n",
            "from side to side. \"Who am I\n",
            "\n",
            "[101 | 235.76] loss=2.79 avg=2.98\n",
            "[102 | 237.98] loss=2.92 avg=2.98\n",
            "[103 | 240.20] loss=2.72 avg=2.97\n",
            "[104 | 242.42] loss=2.83 avg=2.97\n",
            "[105 | 244.65] loss=2.84 avg=2.97\n",
            "[106 | 246.88] loss=2.81 avg=2.97\n",
            "[107 | 249.11] loss=2.97 avg=2.97\n",
            "[108 | 251.33] loss=2.93 avg=2.97\n",
            "[109 | 253.56] loss=2.74 avg=2.96\n",
            "[110 | 255.79] loss=2.82 avg=2.96\n",
            "[111 | 258.02] loss=2.89 avg=2.96\n",
            "[112 | 260.25] loss=2.97 avg=2.96\n",
            "[113 | 262.47] loss=2.80 avg=2.96\n",
            "[114 | 264.70] loss=2.71 avg=2.95\n",
            "[115 | 266.93] loss=2.72 avg=2.95\n",
            "[116 | 269.16] loss=2.80 avg=2.95\n",
            "[117 | 271.40] loss=2.79 avg=2.95\n",
            "[118 | 273.62] loss=2.74 avg=2.94\n",
            "[119 | 275.85] loss=2.85 avg=2.94\n",
            "[120 | 278.07] loss=2.88 avg=2.94\n",
            "[121 | 280.29] loss=2.64 avg=2.94\n",
            "[122 | 282.52] loss=2.86 avg=2.93\n",
            "[123 | 284.75] loss=2.84 avg=2.93\n",
            "[124 | 286.98] loss=2.87 avg=2.93\n",
            "[125 | 289.20] loss=2.67 avg=2.93\n",
            "[126 | 291.42] loss=2.79 avg=2.93\n",
            "[127 | 293.65] loss=2.77 avg=2.92\n",
            "[128 | 295.88] loss=2.93 avg=2.92\n",
            "[129 | 298.10] loss=2.87 avg=2.92\n",
            "[130 | 300.32] loss=2.70 avg=2.92\n",
            "[131 | 302.54] loss=2.66 avg=2.92\n",
            "[132 | 304.76] loss=2.84 avg=2.92\n",
            "[133 | 306.98] loss=2.62 avg=2.91\n",
            "[134 | 309.20] loss=2.88 avg=2.91\n",
            "[135 | 311.42] loss=2.72 avg=2.91\n",
            "[136 | 313.65] loss=2.69 avg=2.91\n",
            "[137 | 315.87] loss=2.74 avg=2.90\n",
            "[138 | 318.10] loss=2.63 avg=2.90\n",
            "[139 | 320.33] loss=2.62 avg=2.90\n",
            "[140 | 322.55] loss=2.67 avg=2.89\n",
            "[141 | 324.77] loss=2.83 avg=2.89\n",
            "[142 | 326.99] loss=2.64 avg=2.89\n",
            "[143 | 329.21] loss=2.71 avg=2.89\n",
            "[144 | 331.44] loss=2.75 avg=2.89\n",
            "[145 | 333.66] loss=2.63 avg=2.88\n",
            "[146 | 335.89] loss=2.79 avg=2.88\n",
            "[147 | 338.10] loss=2.62 avg=2.88\n",
            "[148 | 340.33] loss=2.77 avg=2.88\n",
            "[149 | 342.55] loss=2.75 avg=2.87\n",
            "[150 | 344.78] loss=2.67 avg=2.87\n",
            "[151 | 347.01] loss=2.74 avg=2.87\n",
            "[152 | 349.23] loss=2.71 avg=2.87\n",
            "[153 | 351.46] loss=2.61 avg=2.86\n",
            "[154 | 353.68] loss=2.55 avg=2.86\n",
            "[155 | 355.91] loss=2.60 avg=2.86\n",
            "[156 | 358.13] loss=2.62 avg=2.85\n",
            "[157 | 360.36] loss=2.69 avg=2.85\n",
            "[158 | 362.59] loss=2.68 avg=2.85\n",
            "[159 | 364.82] loss=2.69 avg=2.85\n",
            "[160 | 367.04] loss=2.76 avg=2.85\n",
            "[161 | 369.27] loss=2.54 avg=2.84\n",
            "[162 | 371.50] loss=2.79 avg=2.84\n",
            "[163 | 373.72] loss=2.55 avg=2.84\n",
            "[164 | 375.95] loss=2.66 avg=2.84\n",
            "[165 | 378.17] loss=2.64 avg=2.83\n",
            "[166 | 380.41] loss=2.50 avg=2.83\n",
            "[167 | 382.63] loss=2.65 avg=2.83\n",
            "[168 | 384.86] loss=2.61 avg=2.83\n",
            "[169 | 387.09] loss=2.66 avg=2.82\n",
            "[170 | 389.33] loss=2.75 avg=2.82\n",
            "[171 | 391.56] loss=2.71 avg=2.82\n",
            "[172 | 393.78] loss=2.67 avg=2.82\n",
            "[173 | 396.01] loss=2.58 avg=2.82\n",
            "[174 | 398.23] loss=2.60 avg=2.81\n",
            "[175 | 400.45] loss=2.70 avg=2.81\n",
            "[176 | 402.68] loss=2.68 avg=2.81\n",
            "[177 | 404.91] loss=2.61 avg=2.81\n",
            "[178 | 407.14] loss=2.65 avg=2.81\n",
            "[179 | 409.37] loss=2.53 avg=2.80\n",
            "[180 | 411.59] loss=2.62 avg=2.80\n",
            "[181 | 413.81] loss=2.72 avg=2.80\n",
            "[182 | 416.05] loss=2.62 avg=2.80\n",
            "[183 | 418.28] loss=2.59 avg=2.80\n",
            "[184 | 420.51] loss=2.35 avg=2.79\n",
            "[185 | 422.74] loss=2.36 avg=2.79\n",
            "[186 | 424.96] loss=2.55 avg=2.78\n",
            "[187 | 427.19] loss=2.44 avg=2.78\n",
            "[188 | 429.42] loss=2.54 avg=2.78\n",
            "[189 | 431.65] loss=2.58 avg=2.77\n",
            "[190 | 433.88] loss=2.57 avg=2.77\n",
            "[191 | 436.11] loss=2.56 avg=2.77\n",
            "[192 | 438.33] loss=2.73 avg=2.77\n",
            "[193 | 440.56] loss=2.56 avg=2.77\n",
            "[194 | 442.79] loss=2.50 avg=2.76\n",
            "[195 | 445.02] loss=2.49 avg=2.76\n",
            "[196 | 447.24] loss=2.75 avg=2.76\n",
            "[197 | 449.47] loss=2.75 avg=2.76\n",
            "[198 | 451.70] loss=2.47 avg=2.76\n",
            "[199 | 453.93] loss=2.43 avg=2.75\n",
            "[200 | 456.15] loss=2.54 avg=2.75\n",
            "======== SAMPLE 1 ========\n",
            " man in the yard, she thought, thinking. She was a liar, she'd told myself, I'd raped her, I was a virgin, and I was in Winterfell with my baby brother . . . \n",
            "She felt as if she were going to die, and then she was. \n",
            "Robert was holding her when she struggled hard to reach him. \"Stay here.\" \n",
            "It was not very long before the direwolf began to gape. \"My legs are cut off.\" He reached for his sword to stroke it \n",
            "with a snarl. \"I can't stand to walk.\" \n",
            "Page 6\n",
            "\n",
            "\"You never have, Ned,\" Ned said. \"You're too young for this. It will not work. I have strength left in me.\" \n",
            "\"What will?\" The direwolf wiggled at his blade. \"I do not fear you. Your mother tells me that when you're so young, I \n",
            "will \n",
            "always be like you.\" \n",
            "Ned knew no spells yet, and it was not the first time. The first time he had seen the direwolf, it was when he \n",
            "lost his knife in the mud of a maelstrom. \n",
            "Once he had seen a dead child, a baby. No. The night was night, and there was no light inside. Even \n",
            "when he dressed himself, it was all dark beneath his blankets. \n",
            "And suddenly he remembered. A heartbeat away, a hand on the direwolf's shoulder. \n",
            "He felt as if the wolf were pressing a finger along his cheek. \n",
            "After a second of relief, a third, and it was gone, the direwolf was laying half asleep in the \n",
            "seat of an old mountain \n",
            "horse. \"Oh, damn it,\" he muttered. \n",
            "His eyes flickered in the night, and he saw blood, but no bloody, and no red, and no green. \n",
            "\"What do you have here?\" Robert told him as soon as he was out of earshot. \"My body was stolen \n",
            "on the Hill.\" There was nothing to die of. \n",
            "\"Where are my swords?\" \n",
            "\"What is there to die of?\" \n",
            "\"As they say.\" \n",
            "\"As they say,\" he told them. \"The Trident was never my sword. I had it from Pentos.\" \n",
            "\"A Trident?\" He did not realize how far off to the west he had gone, and how long he had gone. \n",
            "\"As they say in the Trident belt,\" Robert added. He had never said a word of it to Robert, yet he could not help but \n",
            "crawl back to his horse. Robert was a tall and grizzled man who seemed to have spent his last days \n",
            "in prison, and that was after his father had left him, the day he was sent to King's Landing. To his \n",
            "heck, it had been thirty years. \n",
            "\"If Lord Commander Rast had not stolen it, I'm certain no one would have even questioned \n",
            "her. I'm not one to question the honor of a knight.\" \n",
            "Robert was a grey-haired man with a dark face, with a heavy jaw and thick red hair. He did not \n",
            "know Maester Luwin, but he knew the great-grandson of the Lord of the Trident. \"I'd rather not \n",
            "have a lie.\" \n",
            "\"Would you rather be a liar?\" \n",
            "\"I have the honor to,\" Robert said. \"A knight,\" \n",
            "Maester Luwin reminded him, \"my brothers. We call knights from all over the Seven Kingdoms, no matter \n",
            "how small. A lion is one of ours, but how dare you put a sword in a pig's leg? The one we \n",
            "are fighting, I think we'll agree on that one.\" \n",
            "\"It's one I do not care to see,\" Robert admitted. \"I'll have you tell me when you need it most.\" \n",
            "\"No one will question my innocence,\" Maester Luwin assured him. \"The time of my death is in my power. \n",
            "Yet-\" \n",
            "Robert was interrupted by a scream. \"A direwolf!\" Maester Luwin slammed the handle of his \n",
            "saddlebladder into the blackened bench beneath his feet. \"No. A direwolf, no. No. Two names for you, no. \n",
            "Three.\" \n",
            "Maester Luwin's voice was cut off, and he turned away from his horses, his face pale, the tears \n",
            "clotching his brow. \"I cannot leave this dead, Robert. Do you understand?\" \n",
            "Robert glanced up. The direwolf snarled at him, menacingly. \"No one. \" The direwolf barked. The \n",
            "wolf moved to where the direwolf was looking. \"Go to the cell,\" it commanded\n",
            "\n",
            "[201 | 469.51] loss=2.80 avg=2.75\n",
            "[202 | 471.73] loss=2.57 avg=2.75\n",
            "[203 | 473.96] loss=2.56 avg=2.75\n",
            "[204 | 476.19] loss=2.53 avg=2.74\n",
            "[205 | 478.42] loss=2.55 avg=2.74\n",
            "[206 | 480.65] loss=2.61 avg=2.74\n",
            "[207 | 482.88] loss=2.68 avg=2.74\n",
            "[208 | 485.10] loss=2.60 avg=2.74\n",
            "[209 | 487.33] loss=2.48 avg=2.73\n",
            "[210 | 489.56] loss=2.43 avg=2.73\n",
            "[211 | 491.79] loss=2.77 avg=2.73\n",
            "[212 | 494.02] loss=2.67 avg=2.73\n",
            "[213 | 496.24] loss=2.68 avg=2.73\n",
            "[214 | 498.47] loss=2.69 avg=2.73\n",
            "[215 | 500.70] loss=2.56 avg=2.73\n",
            "[216 | 502.93] loss=2.47 avg=2.72\n",
            "[217 | 505.16] loss=2.56 avg=2.72\n",
            "[218 | 507.38] loss=2.51 avg=2.72\n",
            "[219 | 509.61] loss=2.47 avg=2.72\n",
            "[220 | 511.84] loss=2.52 avg=2.72\n",
            "[221 | 514.07] loss=2.60 avg=2.71\n",
            "[222 | 516.30] loss=2.52 avg=2.71\n",
            "[223 | 518.52] loss=2.46 avg=2.71\n",
            "[224 | 520.75] loss=2.49 avg=2.71\n",
            "[225 | 522.98] loss=2.58 avg=2.71\n",
            "[226 | 525.21] loss=2.37 avg=2.70\n",
            "[227 | 527.44] loss=2.53 avg=2.70\n",
            "[228 | 529.67] loss=2.43 avg=2.70\n",
            "[229 | 531.90] loss=2.83 avg=2.70\n",
            "[230 | 534.13] loss=2.70 avg=2.70\n",
            "[231 | 536.35] loss=2.37 avg=2.69\n",
            "[232 | 538.59] loss=2.68 avg=2.69\n",
            "[233 | 540.82] loss=2.62 avg=2.69\n",
            "[234 | 543.05] loss=2.81 avg=2.69\n",
            "[235 | 545.28] loss=2.49 avg=2.69\n",
            "[236 | 547.51] loss=2.51 avg=2.69\n",
            "[237 | 549.74] loss=2.49 avg=2.69\n",
            "[238 | 551.97] loss=2.56 avg=2.69\n",
            "[239 | 554.20] loss=2.25 avg=2.68\n",
            "[240 | 556.43] loss=2.38 avg=2.68\n",
            "[241 | 558.66] loss=2.42 avg=2.68\n",
            "[242 | 560.88] loss=2.67 avg=2.68\n",
            "[243 | 563.12] loss=2.61 avg=2.67\n",
            "[244 | 565.35] loss=2.37 avg=2.67\n",
            "[245 | 567.58] loss=2.54 avg=2.67\n",
            "[246 | 569.80] loss=2.44 avg=2.67\n",
            "[247 | 572.03] loss=2.56 avg=2.67\n",
            "[248 | 574.26] loss=2.28 avg=2.66\n",
            "[249 | 576.50] loss=2.44 avg=2.66\n",
            "[250 | 578.73] loss=2.55 avg=2.66\n",
            "[251 | 580.96] loss=2.34 avg=2.66\n",
            "[252 | 583.18] loss=2.41 avg=2.65\n",
            "[253 | 585.41] loss=2.41 avg=2.65\n",
            "[254 | 587.65] loss=2.66 avg=2.65\n",
            "[255 | 589.88] loss=2.54 avg=2.65\n",
            "[256 | 592.11] loss=2.63 avg=2.65\n",
            "[257 | 594.34] loss=2.75 avg=2.65\n",
            "[258 | 596.57] loss=2.52 avg=2.65\n",
            "[259 | 598.80] loss=2.40 avg=2.65\n",
            "[260 | 601.04] loss=2.42 avg=2.64\n",
            "[261 | 603.26] loss=2.56 avg=2.64\n",
            "[262 | 605.49] loss=2.28 avg=2.64\n",
            "[263 | 607.72] loss=2.34 avg=2.64\n",
            "[264 | 609.94] loss=2.40 avg=2.63\n",
            "[265 | 612.18] loss=2.40 avg=2.63\n",
            "[266 | 614.41] loss=2.52 avg=2.63\n",
            "[267 | 616.63] loss=2.36 avg=2.63\n",
            "[268 | 618.86] loss=2.19 avg=2.62\n",
            "[269 | 621.10] loss=2.46 avg=2.62\n",
            "[270 | 623.33] loss=2.37 avg=2.62\n",
            "[271 | 625.55] loss=2.46 avg=2.62\n",
            "[272 | 627.78] loss=2.34 avg=2.61\n",
            "[273 | 630.00] loss=2.52 avg=2.61\n",
            "[274 | 632.23] loss=2.37 avg=2.61\n",
            "[275 | 634.45] loss=2.25 avg=2.60\n",
            "[276 | 636.68] loss=2.49 avg=2.60\n",
            "[277 | 638.91] loss=2.29 avg=2.60\n",
            "[278 | 641.14] loss=2.37 avg=2.60\n",
            "[279 | 643.37] loss=2.28 avg=2.59\n",
            "[280 | 645.59] loss=2.40 avg=2.59\n",
            "[281 | 647.82] loss=2.58 avg=2.59\n",
            "[282 | 650.05] loss=2.22 avg=2.59\n",
            "[283 | 652.28] loss=2.27 avg=2.58\n",
            "[284 | 654.50] loss=2.35 avg=2.58\n",
            "[285 | 656.73] loss=2.31 avg=2.58\n",
            "[286 | 658.96] loss=2.33 avg=2.58\n",
            "[287 | 661.20] loss=2.31 avg=2.57\n",
            "[288 | 663.43] loss=2.14 avg=2.57\n",
            "[289 | 665.66] loss=2.21 avg=2.57\n",
            "[290 | 667.88] loss=2.45 avg=2.56\n",
            "[291 | 670.11] loss=2.31 avg=2.56\n",
            "[292 | 672.34] loss=2.28 avg=2.56\n",
            "[293 | 674.56] loss=2.75 avg=2.56\n",
            "[294 | 676.79] loss=2.45 avg=2.56\n",
            "[295 | 679.01] loss=2.00 avg=2.55\n",
            "[296 | 681.23] loss=2.20 avg=2.55\n",
            "[297 | 683.46] loss=2.27 avg=2.55\n",
            "[298 | 685.69] loss=2.46 avg=2.55\n",
            "[299 | 687.91] loss=2.47 avg=2.55\n",
            "[300 | 690.14] loss=2.17 avg=2.54\n",
            "======== SAMPLE 1 ========\n",
            " were told.\" Maester Pycelle smiled. His hands were taut and stiff around his throat, and they had been tumbling down the king's arm. It made him look quite a little bit like his grandfather. \n",
            "\"It was a mistake.\" \n",
            "\"Do as I command you.\" Ned pushed away, and Jaime followed, his head spinning wildly. Sansa was at once \n",
            "excited and terrified. She did not know what to do. She was not even sure what to do. \n",
            "She pushed down a hand to protect her eyes. \n",
            "\"You know, the Dothraki had a way of tricking you. When Bran was a boy, his mother died. A dozen women went into the \n",
            "womb, and when he was old enough to marry her, she sent men to rape her. Then in the khalasar of Dorne she sent \n",
            "the Dornishmen back to Braavos in her womb. The gods only wanted me to be like that, the one they wanted, \n",
            "that woman with the red hair, I have never had before. She came before me like that too, but Arya was too young to \n",
            "understand, and she wanted a man as ugly as me to take her. I think it was my sister Myrcella who sent her off.\" \n",
            "Maester Pycelle gave a sigh. \"No, the gods forbid, I will not allow it. I want the Lannisters to rule Westeros.\" \n",
            "The Dornishmen went back to their rooms. Jaime took a deep breath to rise quietly, afraid that if he was \n",
            "offered a ride back from Winterfell he might be frightened enough to take him with him. He had no time to care. \n",
            "\"It was my father who sent you here, my lord.\" \n",
            "\"The Hand,\" Ned repeated. \"I must ride with him, you may be at heart a Stark.\" He looked at Arya and whispered \n",
            "\"Yes.\" \n",
            "Page 481\n",
            "\n",
            "The Dornishmen took an empty seat, and Arya sat on the benches with her black hair pulled back, and the \n",
            "beds and stables and inns and houses that Arya used to call home. She had her nose and lips and teeth \n",
            "scarlet and painted with silver, she had everything her father had set for her. She had her hair, her dress, her voice. \n",
            "\"It was your father who sent me here,\" Maester Pycelle said. \"No wonder I went from boy to man here on the \n",
            "night of my father's funeral. The Lannisters are not good for you, my friend.\" \n",
            "\"I was . . . scared,\" Arya said. \"I didn't know what to feel about it. All I wanted were some \n",
            "food, some rest.\" \n",
            "\"I will dress as you would, my sweet sisters,\" the queen said. \"I will not let you spoil my wine.\" \n",
            "Arya was shivering as they sat around her, and she looked happier when Ned had given her the \n",
            "carrier's letter she'd needed after she'd taken Arya in. \"You are too kind, Arya,\" she said. \"You are not the \n",
            "daughter I took from my brother Jaime.\" \n",
            "All she wanted was some place to sleep, but Arya had no place in that. She wanted Arya to dream of \n",
            "my brother. She wanted to hold her breath. \"My sister Arya,\" she whispered. \"Mother.\" When \n",
            "mother did not answer, she said, \"I'm not dreaming, I'm dreaming.\" She went to her bedchamber. \"Joss, \" she \n",
            "called, and her eyes closed. The door was there, black and iron, and Arya wanted to run or run away. She \n",
            "wondered if Ned would see her there, and tried to think of any other way to get away. She wrote \n",
            "a letter to her grandfather, telling him that I was not welcome to the castle at Jogher's Day. \n",
            "After she had gone, she had to find a place to sleep. She was a little dizzy on her steps when Ned had \n",
            "closed her door. She'd slept in a small crib near the fire, covered by blankets and grass. The only light \n",
            "suddenly came as she came down the steps. \"Did \n",
            "you see my lord father?\" Arya had asked the gods only once, before. \n",
            "\"No, I didn't,\" Ned said bitterly. \"Where do you think he came?\" \n",
            "Arya found her voice muffled. \"Arya, what is it? I can't hear you, \n",
            "I can't.\" \n",
            "\"You could,\" Ned said, \"but not you.\" \n",
            "\"What?\" \n",
            "\"You can't\n",
            "\n",
            "[301 | 703.23] loss=2.28 avg=2.54\n",
            "[302 | 705.46] loss=2.12 avg=2.53\n",
            "[303 | 707.68] loss=2.19 avg=2.53\n",
            "[304 | 709.91] loss=2.35 avg=2.53\n",
            "[305 | 712.13] loss=2.46 avg=2.53\n",
            "[306 | 714.35] loss=2.40 avg=2.53\n",
            "[307 | 716.58] loss=2.29 avg=2.52\n",
            "[308 | 718.80] loss=2.22 avg=2.52\n",
            "[309 | 721.02] loss=2.56 avg=2.52\n",
            "[310 | 723.24] loss=2.20 avg=2.52\n",
            "[311 | 725.47] loss=2.34 avg=2.52\n",
            "[312 | 727.69] loss=2.42 avg=2.52\n",
            "[313 | 729.91] loss=2.52 avg=2.52\n",
            "[314 | 732.14] loss=2.44 avg=2.51\n",
            "[315 | 734.37] loss=2.21 avg=2.51\n",
            "[316 | 736.60] loss=2.46 avg=2.51\n",
            "[317 | 738.82] loss=2.51 avg=2.51\n",
            "[318 | 741.04] loss=2.50 avg=2.51\n",
            "[319 | 743.27] loss=2.34 avg=2.51\n",
            "[320 | 745.50] loss=2.34 avg=2.51\n",
            "[321 | 747.73] loss=2.17 avg=2.50\n",
            "[322 | 749.95] loss=2.27 avg=2.50\n",
            "[323 | 752.17] loss=2.10 avg=2.50\n",
            "[324 | 754.40] loss=1.91 avg=2.49\n",
            "[325 | 756.62] loss=2.09 avg=2.49\n",
            "[326 | 758.85] loss=2.41 avg=2.49\n",
            "[327 | 761.07] loss=2.02 avg=2.48\n",
            "[328 | 763.31] loss=2.42 avg=2.48\n",
            "[329 | 765.53] loss=2.06 avg=2.48\n",
            "[330 | 767.75] loss=2.30 avg=2.47\n",
            "[331 | 769.99] loss=2.13 avg=2.47\n",
            "[332 | 772.22] loss=2.35 avg=2.47\n",
            "[333 | 774.45] loss=2.38 avg=2.47\n",
            "[334 | 776.68] loss=2.27 avg=2.47\n",
            "[335 | 778.90] loss=2.45 avg=2.47\n",
            "[336 | 781.12] loss=1.98 avg=2.46\n",
            "[337 | 783.36] loss=2.32 avg=2.46\n",
            "[338 | 785.58] loss=2.22 avg=2.46\n",
            "[339 | 787.81] loss=2.49 avg=2.46\n",
            "[340 | 790.04] loss=2.55 avg=2.46\n",
            "[341 | 792.27] loss=2.25 avg=2.46\n",
            "[342 | 794.50] loss=2.26 avg=2.45\n",
            "[343 | 796.73] loss=2.08 avg=2.45\n",
            "[344 | 798.96] loss=2.21 avg=2.45\n",
            "[345 | 801.19] loss=1.93 avg=2.44\n",
            "[346 | 803.42] loss=1.77 avg=2.44\n",
            "[347 | 805.65] loss=2.42 avg=2.44\n",
            "[348 | 807.89] loss=2.33 avg=2.43\n",
            "[349 | 810.11] loss=2.20 avg=2.43\n",
            "[350 | 812.34] loss=2.05 avg=2.43\n",
            "[351 | 814.57] loss=2.16 avg=2.43\n",
            "[352 | 816.79] loss=2.24 avg=2.42\n",
            "[353 | 819.03] loss=2.15 avg=2.42\n",
            "[354 | 821.26] loss=2.30 avg=2.42\n",
            "[355 | 823.49] loss=2.17 avg=2.42\n",
            "[356 | 825.71] loss=2.05 avg=2.41\n",
            "[357 | 827.94] loss=2.17 avg=2.41\n",
            "[358 | 830.17] loss=2.07 avg=2.41\n",
            "[359 | 832.41] loss=2.40 avg=2.41\n",
            "[360 | 834.63] loss=2.46 avg=2.41\n",
            "[361 | 836.86] loss=2.02 avg=2.40\n",
            "[362 | 839.09] loss=2.30 avg=2.40\n",
            "[363 | 841.31] loss=2.15 avg=2.40\n",
            "[364 | 843.54] loss=2.11 avg=2.40\n",
            "[365 | 845.78] loss=2.26 avg=2.40\n",
            "[366 | 848.01] loss=2.26 avg=2.39\n",
            "[367 | 850.24] loss=2.13 avg=2.39\n",
            "[368 | 852.47] loss=2.16 avg=2.39\n",
            "[369 | 854.70] loss=2.16 avg=2.39\n",
            "[370 | 856.94] loss=2.21 avg=2.38\n",
            "[371 | 859.16] loss=2.22 avg=2.38\n",
            "[372 | 861.39] loss=2.25 avg=2.38\n",
            "[373 | 863.62] loss=1.98 avg=2.38\n",
            "[374 | 865.85] loss=2.11 avg=2.37\n",
            "[375 | 868.08] loss=2.04 avg=2.37\n",
            "[376 | 870.30] loss=2.25 avg=2.37\n",
            "[377 | 872.53] loss=2.02 avg=2.37\n",
            "[378 | 874.75] loss=2.00 avg=2.36\n",
            "[379 | 876.98] loss=2.14 avg=2.36\n",
            "[380 | 879.21] loss=2.03 avg=2.36\n",
            "[381 | 881.45] loss=1.88 avg=2.35\n",
            "[382 | 883.67] loss=1.83 avg=2.35\n",
            "[383 | 885.90] loss=2.25 avg=2.35\n",
            "[384 | 888.12] loss=2.24 avg=2.34\n",
            "[385 | 890.34] loss=2.27 avg=2.34\n",
            "[386 | 892.58] loss=2.39 avg=2.34\n",
            "[387 | 894.80] loss=2.28 avg=2.34\n",
            "[388 | 897.03] loss=2.15 avg=2.34\n",
            "[389 | 899.25] loss=2.22 avg=2.34\n",
            "[390 | 901.48] loss=2.44 avg=2.34\n",
            "[391 | 903.70] loss=2.19 avg=2.34\n",
            "[392 | 905.93] loss=2.20 avg=2.34\n",
            "[393 | 908.15] loss=2.17 avg=2.34\n",
            "[394 | 910.38] loss=2.17 avg=2.34\n",
            "[395 | 912.60] loss=2.00 avg=2.33\n",
            "[396 | 914.82] loss=1.93 avg=2.33\n",
            "[397 | 917.06] loss=2.06 avg=2.32\n",
            "[398 | 919.28] loss=2.16 avg=2.32\n",
            "[399 | 921.50] loss=2.03 avg=2.32\n",
            "[400 | 923.73] loss=1.98 avg=2.32\n",
            "======== SAMPLE 1 ========\n",
            " but a large iron-whip with a lion's head in the star charts. \"The night will come when the gods will strike again, in a hundred swift strokes.\" \n",
            "I \n",
            "I had made a vow not to speak of it. How often I spoke of it, I could never say, but I did not \n",
            "understand. Ser Addam ignored me, and the first thing he said was that he was sorry to hear the matter, but he \n",
            "did not comprehend why I had gone. \n",
            "Yet my own words ring clear now. I had fought with the gods in the woods. I had cried. I had kissed the sweet \n",
            "sweet flowers. I had taken all the words men gave you in grief and dread and fear and \n",
            "anger, and yet I knew I had not won victory. I stood up, brave and defiant, and for a time I \n",
            "stood. Yet it was only a voice. I opened my mouth, and for a second there was nothing but silence, and \n",
            "I wanted to cry again, I wanted to be strong again. I wanted to pray that Ser Addam would heal me. I wanted to \n",
            "have a look around the castle, to see how tall the Lord Commander's mans surrounded by \n",
            "great walls had grown beneath them. I wanted to look in the window, to hear the sounds of birds, the beat of distant \n",
            "bells. I wanted to hear the birds howling. I wanted to sit high and sing, to pray that Maester Colemon might hear \n",
            "me. But I knew that if I prayed it would mean my death. \n",
            "I prayed that the Lord Commander would see me, that he might see and forgive me for my words, that he might \n",
            "grant me a hundred years of hard House to follow on my head. I prayed that he might remember me, that \n",
            "he might honor me as a brother. \n",
            "And yet, it seemed as if I was not going to die a simple death. I wanted to be free. I wanted to go home. I \n",
            "brought my dead brother myself. I took him along with me, with the rest of my men. I took him back after \n",
            "I had prayed that he might see me again. And now I lay dying. I sit like a man dead, \n",
            "feeling the cold inside. I sit sad. I lay still, feeling the emptiness as I sit sad, wondering what I \n",
            "would do if I told anyone. I lay still, afraid for my own life and my lord's. I sit as mute as \n",
            "a child in a dark room, afraid to speak to anything. \n",
            "What would Ser Addam have done? He would have told me, if any man would listen. He would have \n",
            "said my name, of course. That I must have that ugly black hair of his. Even as a boy I hated it. I loved \n",
            "him so much that I said, I hate . . . maybe it was just one of the many reasons I hated the black. I don't \n",
            "know. I was just a child, I don't know. The only thing I could think of was that I must have him \n",
            "and be free. I wanted to sit the sand and watch that dark wood with all my might, but somehow I felt \n",
            "sick. Even a year older than I was. I never had the strength to sit up straight because I just sat there, and even \n",
            "when I wanted to I had to sit. I remember being so much taller than I was, and the pain was so \n",
            "painful. I woke up and was so cold. I had a fever all over my body, and a cold sore around \n",
            "my throat. Every fortnight I would lie awake and dream I had a body like that, like some terrible \n",
            "death-dream, and then I would begin to dream again. I didn't know how I got away. I was afraid to walk \n",
            "anything but guard, afraid that it would tear me like the rest of the earth. I remember waking in the \n",
            "wet and feeling like \n",
            "a cot. It was the sound of someone screaming, but I could not make out what. What was I? \n",
            "I was afraid for my own life. For the first time in ten years, I decided to sit. \n",
            "I was afraid to open my eyes. I tried to open my hair. I tried to open my eyes, to see what it was \n",
            "like, to smell it, to feel it. I tried to dress until it made no sense to go to the library or \n",
            "the Town Hall. \n",
            "But just as suddenly as my eyes opened to the ceiling, a great grey wolf moved in the air. I \n",
            "could smell the warmth of it with my own nose. I smelled blood as well; it was as if I were seeing water. \n",
            "\n",
            "[401 | 936.87] loss=1.56 avg=2.31\n",
            "[402 | 939.09] loss=2.00 avg=2.31\n",
            "[403 | 941.32] loss=1.92 avg=2.30\n",
            "[404 | 943.55] loss=1.90 avg=2.30\n",
            "[405 | 945.78] loss=2.03 avg=2.30\n",
            "[406 | 948.00] loss=1.92 avg=2.29\n",
            "[407 | 950.23] loss=2.13 avg=2.29\n",
            "[408 | 952.46] loss=2.17 avg=2.29\n",
            "[409 | 954.69] loss=2.04 avg=2.29\n",
            "[410 | 956.92] loss=1.85 avg=2.28\n",
            "[411 | 959.14] loss=2.11 avg=2.28\n",
            "[412 | 961.37] loss=1.98 avg=2.28\n",
            "[413 | 963.60] loss=1.83 avg=2.27\n",
            "[414 | 965.83] loss=1.72 avg=2.27\n",
            "[415 | 968.06] loss=2.18 avg=2.27\n",
            "[416 | 970.29] loss=2.32 avg=2.27\n",
            "[417 | 972.51] loss=1.71 avg=2.26\n",
            "[418 | 974.74] loss=2.04 avg=2.26\n",
            "[419 | 976.98] loss=1.92 avg=2.26\n",
            "[420 | 979.21] loss=2.16 avg=2.25\n",
            "[421 | 981.43] loss=1.78 avg=2.25\n",
            "[422 | 983.66] loss=1.95 avg=2.25\n",
            "[423 | 985.90] loss=1.70 avg=2.24\n",
            "[424 | 988.14] loss=2.21 avg=2.24\n",
            "[425 | 990.37] loss=2.11 avg=2.24\n",
            "[426 | 992.60] loss=1.85 avg=2.24\n",
            "[427 | 994.83] loss=2.29 avg=2.24\n",
            "[428 | 997.05] loss=1.67 avg=2.23\n",
            "[429 | 999.28] loss=1.82 avg=2.23\n",
            "[430 | 1001.52] loss=1.92 avg=2.22\n",
            "[431 | 1003.76] loss=2.21 avg=2.22\n",
            "[432 | 1005.99] loss=1.97 avg=2.22\n",
            "[433 | 1008.22] loss=2.12 avg=2.22\n",
            "[434 | 1010.45] loss=1.78 avg=2.21\n",
            "[435 | 1012.69] loss=2.14 avg=2.21\n",
            "[436 | 1014.93] loss=2.24 avg=2.21\n",
            "[437 | 1017.16] loss=2.20 avg=2.21\n",
            "[438 | 1019.39] loss=2.01 avg=2.21\n",
            "[439 | 1021.62] loss=1.91 avg=2.21\n",
            "[440 | 1023.84] loss=2.10 avg=2.21\n",
            "[441 | 1026.08] loss=1.97 avg=2.21\n",
            "[442 | 1028.31] loss=2.20 avg=2.21\n",
            "[443 | 1030.53] loss=2.06 avg=2.20\n",
            "[444 | 1032.77] loss=1.84 avg=2.20\n",
            "[445 | 1035.00] loss=1.65 avg=2.19\n",
            "[446 | 1037.23] loss=1.83 avg=2.19\n",
            "[447 | 1039.46] loss=2.10 avg=2.19\n",
            "[448 | 1041.69] loss=2.01 avg=2.19\n",
            "[449 | 1043.91] loss=2.00 avg=2.19\n",
            "[450 | 1046.14] loss=2.09 avg=2.19\n",
            "[451 | 1048.37] loss=1.71 avg=2.18\n",
            "[452 | 1050.60] loss=1.95 avg=2.18\n",
            "[453 | 1052.83] loss=1.85 avg=2.17\n",
            "[454 | 1055.06] loss=1.92 avg=2.17\n",
            "[455 | 1057.28] loss=1.90 avg=2.17\n",
            "[456 | 1059.50] loss=1.77 avg=2.17\n",
            "[457 | 1061.74] loss=1.80 avg=2.16\n",
            "[458 | 1063.97] loss=1.67 avg=2.16\n",
            "[459 | 1066.20] loss=2.12 avg=2.16\n",
            "[460 | 1068.42] loss=1.82 avg=2.15\n",
            "[461 | 1070.65] loss=1.85 avg=2.15\n",
            "[462 | 1072.87] loss=2.00 avg=2.15\n",
            "[463 | 1075.10] loss=1.85 avg=2.15\n",
            "[464 | 1077.33] loss=1.71 avg=2.14\n",
            "[465 | 1079.56] loss=1.32 avg=2.13\n",
            "[466 | 1081.79] loss=1.71 avg=2.13\n",
            "[467 | 1084.02] loss=1.57 avg=2.12\n",
            "[468 | 1086.26] loss=1.68 avg=2.12\n",
            "[469 | 1088.48] loss=2.03 avg=2.12\n",
            "[470 | 1090.70] loss=1.49 avg=2.11\n",
            "[471 | 1092.93] loss=2.12 avg=2.11\n",
            "[472 | 1095.16] loss=1.69 avg=2.11\n",
            "[473 | 1097.39] loss=1.75 avg=2.10\n",
            "[474 | 1099.61] loss=1.62 avg=2.10\n",
            "[475 | 1101.84] loss=1.88 avg=2.10\n",
            "[476 | 1104.07] loss=1.82 avg=2.09\n",
            "[477 | 1106.29] loss=1.90 avg=2.09\n",
            "[478 | 1108.52] loss=1.93 avg=2.09\n",
            "[479 | 1110.75] loss=2.05 avg=2.09\n",
            "[480 | 1112.98] loss=1.70 avg=2.09\n",
            "[481 | 1115.20] loss=2.22 avg=2.09\n",
            "[482 | 1117.42] loss=2.26 avg=2.09\n",
            "[483 | 1119.65] loss=1.67 avg=2.08\n",
            "[484 | 1121.88] loss=1.92 avg=2.08\n",
            "[485 | 1124.10] loss=1.72 avg=2.08\n",
            "[486 | 1126.33] loss=2.00 avg=2.08\n",
            "[487 | 1128.55] loss=1.84 avg=2.08\n",
            "[488 | 1130.78] loss=1.87 avg=2.07\n",
            "[489 | 1133.00] loss=1.75 avg=2.07\n",
            "[490 | 1135.24] loss=2.00 avg=2.07\n",
            "[491 | 1137.46] loss=1.64 avg=2.07\n",
            "[492 | 1139.68] loss=1.75 avg=2.06\n",
            "[493 | 1141.90] loss=1.76 avg=2.06\n",
            "[494 | 1144.12] loss=1.70 avg=2.06\n",
            "[495 | 1146.35] loss=1.86 avg=2.05\n",
            "[496 | 1148.58] loss=1.91 avg=2.05\n",
            "[497 | 1150.80] loss=1.73 avg=2.05\n",
            "[498 | 1153.03] loss=1.80 avg=2.05\n",
            "[499 | 1155.25] loss=1.91 avg=2.05\n",
            "[500 | 1157.48] loss=1.63 avg=2.04\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1672561a-10a0-4bb7-c77a-855de0c8ed2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? No, my lord, no, there is no other.\" \n",
            "\"The very first,\" Lord Randyll said, \"our last.\" \n",
            "\"It is not for you to tell them what to do,\" Varys said. \"It is for the Dothraki-\" \n",
            "\"You will call them,\" Lord Randyll said. \"I will not leave you unguarded, my lords. I promise you this, \n",
            "they will hear.\" \n",
            "\"I am not bound to them,\" Varys said. \"I have no need of their help. Give them what you \n",
            "want.\" \n",
            "Page 272\n",
            "\n",
            "His face was grave as he laid the letter before them. \"As you wish.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}